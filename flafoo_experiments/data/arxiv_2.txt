XLNet: Generalized Autoregressive Pretraining Language Understanding. modeling bidirectional contexts, denoising autoencoding pretraining BERT autoregressive language modeling. masks, BERT pretrain-finetune discrepancy. XLNet, generalized autoregressive pretraining learning bidirectional contexts BERT autoregressive formulation. XLNet ransformer-XL, autoregressive model, into pretraining. XLNet outperforms BERT question answering, natural language inference, sentiment analysis, document ranking. Bert transformer stack of LSTMs
RoBERTa: Robustly Optimized BERT Pretraining. hyperparameter replication study of BERT pretraining key hyperparameters and training data size. BERT undertrained. state-of-the-art results GLUE, RACE, SQuAD. BERT undertrained and improved training BERT models, RoBERTa, post-BERT.
Ordered Neurons: Tree Structures Recurrent Neural Networks. Natural language hierarchically structured: LSTM architecture neurons track information at different time scales, recurrent architecture, ordered neurons LSTM (ON-LSTM), language modeling, unsupervised parsing, targeted syntactic evaluation, and logical inference.
BERT: Pre-training Deep Bidirectional Transformers Language Understanding: language representation model BERT, Bidirectional Encoder Representations Transformers. language representation models, BERT pre-train deep bidirectional representations unlabeled text. pre-trained BERT model state-of-the-art, question answering and language inference. BERT state-of-the-art natural language processing tasks.
RelationNet++: Object Detection via Transformer Decoder. Existing object detection, i.e., rectangle boxes RetinaNet Faster R-CNN, center points FCOS and RepPoints, and corner points CornerNet. classification finer localization, heterogeneous non-grid feature extraction. attention-based decoder Transformer object detector. vanilla detectors. bridging visual representations (BVR). in-place object detection frameworks, RetinaNet, Faster R-CNN, FCOS and ATSS. RelationNet++.
Graph-based Topic Extraction Vector Embeddings Text Documents: Corpus of News Articles: unstructured corpora `topics' vector embeddings natural language processing graph partitioning priori clusters corpus. graph-based clustering clustering topic modelling, text vector embeddings, Bag-of-Words Doc2Vec transformers Bert. corpus.
Peak Detection Data Independent Acquisition Mass Spectrometry Data Semisupervised Convolutional Transformers: Liquid Chromatography Mass Spectrometry (LC-MS) proteome (i.e. proteins ). LC-MS time series spectrum, chromatography. elution peptide ion traces (extracted ion chromatograms, or XICs).peak detection multivariate time series segmentation problem Transformer architecture. Here we augment Transformers, long distance dependencie, Convolutional Neural Networks (CNNs), local context, Transformers with Convolutional Self-Attention. semisupervised manner semisupervised image classification multi-channel time series data. neural network architectures automated peak detection.
ReadOnce Transformers: Text Transformers: language models extract information. transformer-based approach, ReadOnce Transformers, information-capturing text. text-to-text ReadOnce Representations multi-hop QA, abstractive QA, and summarization. text-to-text models, documents.
Inducing Taxonomic Knowledge Pretrained Transformers: taxonomic trees pretrained transformers maximum spanning tree. train finetuning it parent-child relations subtrees of WordNet and test on non-overlapping subtrees. In addition, we incorporate semi-structured subtrees of WordNet.
Multi-Unit Transformers Neural Machine Translation: Transformer Neural Machine Translation. Transformer stacking combination of Multihead Attentions and FFN, multiple parallel attention. Multi-Unit Transformers (MUTE), Transformer. machine translation tasks.
Transformers: Automatic Corpus Creation: Transformers state-of-the-art in Natural Language Processing (NLP). pretrained transformers, Natural Language Inference (NLI). NewsPH-NLI, ELECTRA.
Measuring Systematic Generalization Neural Proof Generation Transformers: Transformer language models (TLMs) natural language. systematic generalization natural language, TLMs backward-chaining.
Parameter Norm Growth During Training of Transformers: gradient descent (GD). bias, transformer parameters grow magnitude training. GD l2 norm threshold training-set accuracy.T5. pretrained T5 semi-discretized network activation functions. GD Natural language processing (NLP). transformers, feedforward ReLU networks.
Deep Transformers with Latent Depth: The Transformer model state-of-the-art sequence modeling tasks. probabilistic framework layers posterior distributions. Transformer network multilingual machine translation posteriors for each language pair. vanishing gradient deep Transformers. WMT English-German machine translation and masked language modeling, deeper Transformers. multilingual machine translation many-to-one and one-to-many translation with diverse language pairs.
Empirical Study Transformers Source Code: natural language processing (NLP), Transformers source code processing, source code and text. natural language, source code structured, syntax programming language. Transformer modifications syntactic information. Transformers syntactic information. three tasks (code completion, function naming and bug fixing) and syntax-capturing modifications. meaningful predictions syntactic information underline syntactic information.
Calibration of Pre-trained Transformers: Pre-trained Transformers natural language processing. BERT RoBERTa, natural language inference, paraphrase detection, and commonsense reasoning. out-of-domain settings. calibration.
Subjective Question Answering: inner workings of Transformers: subjectivity. sentiment opinion mining. Question Answering, SubjQA. SubjQA QA dataset subjective opinions subjective questions, opinions and process sentiment natural language utterances, inner workings (latent representations) Transformer-based architecture better understanding "black-box" models. Transformer's hidden representations, answer span, clustered vector space erroneous predictions. objective and subjective questions. probability high cosine similarity among hidden representations latent space answer span predictions.
Assessing Phrasal Representation Composition Transformers: Deep transformer NLP tasks, linguistic inputs. systematic analysis phrasal representations state-of-the-art pre-trained transformers.
Pretrained Transformers for Text Ranking: BERT and Beyond: text ranking is search, natural language processing. text ranking transformers, BERT. transformers self-supervised pretraining, natural language processing (NLP), information retrieval (IR). transformers ranking problems transformer models reranking multi-stage ranking learned dense representations perform ranking. handling long documents, sentence-by-sentence NLP, result quality efficiency query latency. transformer architectures pretraining techniques, text ranking. open research questions, pretrained transformers text ranking.
Dissecting Lottery Ticket Transformers: Structural and Behavioral Study of Sparse Neural Machine Translation: lottery ticket hypothesis sparse Transformers NMT BLEU. Transformers semantic information. sparse models. Attention mechanisms remain remarkably consistent sparsity.
Computational Power Transformers Implications Sequence Modeling: Transformers sequence modeling tasks. inner workings of Transformers. Transformers positional encodings, attention heads, residual connections, feedforward networks. vanilla Transformers Turing-complete Transformers positional masking. residual connection. machine translation and synthetic tasks.
Query-Key Normalization Transformers: Low-resource language translation NLP. Transformer's normalization QKNorm, normalization technique attention mechanism softmax function. key matrix prior to multiplying learnable parameter embedding dimension.
EdinburghNLP at WNUT-2020 Task 2: Leveraging Transformers Generalized Augmentation. WNUT-2020 Task 2: transformers RoBERTa, XLNet, and BERTweet semi-supervised. fasttext embeddings.
Performance evaluation application computation low-cost homogeneous machine learning model algorithm image classification: The image classification machine learning model. state-of-the-art ensemble model methodologies.
LiteDepthwiseNet: Lightweight Network Hyperspectral Image Classification: Deep learning hyperspectral image (HSI) classification, high accuracy compared traditional methods. network architecture, LiteDepthwiseNet, HSI classification. Based on 3D depthwise convolution, LiteDepthwiseNet standard convolution depthwise convolution pointwise convolution, high classification performance with minimal parameters. ReLU layer Batch Normalization layer original 3D depthwise convolution, overfitting phenomenon. model's attention, cross-entropy loss balanced cross-entropy loss. LiteDepthwiseNet state-of-the-art.
Satellite Image Classification with Deep Learning: Satellite imagery, law enforcement, and environmental monitoring. manual identification of objects imagery. geographic expanses. object detection classification algorithms. Deep learning machine learning automation. image understanding convolutional neural networks. object reconition facility recognition high-resolution, multi-spectral satellite imagery. deep learning system classifying objects facilities IARPA Functional Map (fMoW) dataset classes. ensemble convolutional neural networks and additional neural networks integrate satellite metadata image features. Python Keras TensorFlow deep-learning-libraries Linux server NVIDIA-Titan-X graphics card.
CC-Loss: Channel Correlation Loss Image Classification: loss function classification cross-entropy loss. intra-class and inter-class constraints. loss functions feature distribution model structure. channel correlation loss (CC-Loss) classes channels intra-class inter-class. CC-Loss channel attention module channel attention. Euclidean distance matrix channel attention vectors. feature embedding intra-class compactness inter-class separability. CC-Loss state-of-the-art loss three image classification datasets.
Bounding Boxes: Street View Image Classification Context Encoding Detected Buildings: Street view images classification classification models visual features. Detector-Encoder-Classifier. image-level models convolutional neural networks (CNNs), bounding boxes buildings street view images detector. contextual information co-occurrence patterns (Context encOding of Detected buildINGs). bounding box metadata recurrent neural network (RNN). dual-labeled dataset named "BEAUTY" (Building-dEtection-And-Urban-funcTionalzone-portraYing).  multi-class building detection. "BEAUTY" (Building-dEtection-And-Urban-funcTionalzone-portraYing) CNN.
Glance and Focus: Dynamic Approach Reducing Spatial Redundancy Image Classification: deep convolutional neural networks (CNNs) high resolution images. regions image novel framework image classification sequence strategically selected from the original image with reinforcement learning. state-of-the-art light-weighted CNNs (such as MobileNets, EfficientNets and RegNets). ImageNet deep models. average latency MobileNet-V3 iPhone XS Max sacrificing accuracy.
Semantic video segmentation autonomous driving: semantic video segmentation autonomous driving, road detection real-time video. fully convolutional network.
Centroid Loss Weakly Supervised Semantic Segmentation Quality Control Inspection Application: Process automation accuracy productivity, machine vision system. semantic segmentation solution. vessel corrosion detection, detection system quality control surgery toolboxes sterilization. pixel-level ground truth, weakly supervised annotations (scribbles). clustering approach semantic segmentation network, weakly supervised annotations. vessels' structure hospital surgery toolboxes. weak annotations.
Multi-Attention-Network Semantic Segmentation High-Resolution Remote Sensing Images: Semantic segmentation, yield estimation, and economic assessment. semantic segmentation remote sensing images convolutional neural networks. encoder-decoder architectures U-Net. long-range dependencies of feature maps, semantic. dot-product attention semantic segmentation long-range dependencies, attention impede attention. Multi-Attention-Network (MANet), contextual dependencies multi efficient attention mechanisms. A novel attention mechanism named kernel attention with linear complexity is proposed to alleviate the high computational demand of attention. kernel attention channel attention, feature maps ResNeXt-101 with their corresponding global dependencies, interdependent channel maps. MANet transcends the DeepLab V3+, PSPNet, FastFCN.
Domain Adaptation LiDAR Semantic Segmentation: LiDAR semantic segmentation 3D semantic. data distribution. LiDAR semantic segmentation. learning-based approach distribution semantic classes.
Importance-Aware Semantic Segmentation Self-Driving Discrete Wasserstein Training: Semantic segmentation (SS) self-driving cars, robotics, classifies pixel pre-determined. cross entropy (CE) mean Intersection-over Union (mIoU). cross entropy loss self-driving. segmentation. importance-aware inter-class correlation Wasserstein training framework ground distance matrix. The ground distance matrix pre-defined priori, importance-ignored. From an optimization perspective pre-defined ground distance. CamVid Cityscapes datasets SegNet, ENet, FCN and Deeplab). Wasserstein loss superior segmentation performance safe-driving.
Dense Dual-Path Network for Real-time Semantic Segmentation: Semantic segmentation. Dense Dual-Path Network (DDPNet) real-time semantic segmentation. dense connectivity network Dual-Path-Module (DPM) multi-scale contexts. high-resolution feature maps segmentation output upsampling. DDPNet. SCityscapes dataset, DDPNet mIoU FPS GTX 1080Ti. DDPNet accuracy.
HCNet: Hierarchical Context Network Semantic Segmentation: Global context visual understanding, pixel-level semantic segmentation. self-attention mechanism global context. pixels weak feature correlation. Modeling pixel-level correlation matrix extremely redundant self-attention mechanism. hierarchical context network homogeneous pixels correlations heterogeneous pixels weak correlations. multi-scale guided pre-segmentation feature map classed-based homogeneous regions. homogeneous region, pixel context module pixel-level correlations. different self-attention mechanism models weak heterogeneous correlations dense pixel-level manner, region context module model sparse region-level dependencies region. aggregating fine-grained pixel context features and coarse-grained region context features, our proposed network can not only hierarchically model global context information but also harvest multi-granularity representations to more robustly identify multi-scale objects. We evaluate our approach on Cityscapes and the ISPRS Vaihingen dataset. Without Bells or Whistles, our approach realizes a mean IoU of 82.8% and overall accuracy of 91.4% on Cityscapes and ISPRS Vaihingen test set, achieving state-of-the-art results.
PseudoSeg: Pseudo Labels Semantic Segmentation: semi-supervised learning (SSL) consistency regularization pseudo-labeling image classification low-data regime. classification, semantic segmentation labeling costs. data-efficient training methods. structured outputs segmentation designing pseudo-labeling augmentation SSL strategies. pseudo-labeling well-calibrated pseudo labels weakly-labeled data. pseudo-labeling strategy. pseudo-labeling strategy. pseudo labels data augmentation consistency training segmentation.
Noisy-LSTM: Temporal Awareness Video Semantic Segmentation: Semantic video segmentation. Noisy-LSTM, convolutional LSTMs (ConvLSTMs) temporal coherency video frames. frame video sequence noises. strategy spoils temporal coherency video frames training temporal links ConvLSTMs, feature extraction video frames, regularizer overfitting, annotation computational. state-of-the-art performances CityScapes EndoVis2018 datasets.
Comprehensive Analysis Weakly-Supervised Semantic Segmentation Image Domains: weakly-supervised semantic segmentation pixel classes image labels. image annotations generate, weak supervision segmentation algorithms. background separation partial segmentation problems presented natural scene images and unclear transferred domains characteristics, histopathology satellite images. state-of-the-art weakly-supervised semantic segmentation methods natural scene, histopathology, satellite image datasets. histopathology satellite images weakly-supervised semantic segmentation natural scene images, ambiguous boundaries class co-occurrence. weakly-supervised semantic segmentation.
Semantic Editing Segmentation Map Multi-Expansion Loss: Semantic editing segmentation map interface image generation, image generation. segmentation map semantic inputs. adversarial losses higher image quality, boundary area. MExGAN semantic editing segmentation map, Multi-Expansion (MEx) loss adversarial losses MEx areas. MEx area mask area boundary context. MEx loss, Approximated MEx (A-MEx) loss. semantic editing segmentation map, MExGAN image training data. Extensive experiments semantic editing segmentation map natural image inpainting.
Auto Seg-Loss: Searching Metric Surrogates Semantic Segmentation: searching surrogate losses mainstream semantic segmentation. existing loss functions individual metrics. Extensive experiments PASCAL VOC Cityscapes.
Semi-Supervised Semantic Segmentation Earth Observation: semi-supervised learning. raw image data. large databases. Earth Observation surface coverage, scenes restricted classes. novel large-scale dataset semi-supervised semantic segmentation Earth Observation, MiniFrance suite. MiniFrance high resolution aerial images, climates, landscapes, urban countryside scenes; semantics. MiniFrance dataset semi-supervised learning: labeled unlabeled images training partition, life-like scenario. data representativeness analysis appearance similarity MiniFrance data, semi-supervised. semi-supervised deeparchitectures multi-task learning MiniFrance.
Encoder-decoder semantic segmentation models for electroluminescence images of thin-film photovoltaic modules: We consider a series of image segmentation methods based on the deep neural networks in order to perform semantic segmentation of electroluminescence (EL) images of thin-film modules. We utilize the encoder-decoder deep neural network architecture. The framework is general such that it can easily be extended to other types of images (e.g. thermography) or solar cell technologies (e.g. crystalline silicon modules). The networks are trained and tested on a sample of images from a database with 6000 EL images of Copper Indium Gallium Diselenide (CIGS) thin film modules. We selected two types of features to extract, shunts and so called "droplets". The latter feature is often observed in the set of images. Several models are tested using various combinations of encoder-decoder layers, and a procedure is proposed to select the best model. We show exemplary results with the best selected model. Furthermore, we applied the best model to the full set of 6000 images and demonstrate that the automated segmentation of EL images can reveal many subtle features which cannot be inferred from studying a small sample of images. We believe these features can contribute to process optimization and quality control.
Robust Consistent Estimation of Word Embedding fine-tuning Word2Vec Model: Word embedding vector representation syntactical semantic characteristics natural language processing. deep learning based models for the vectorization word2vec, fasttext, gensim, glove. word2vec hyper-parameters. fine-tuning word2vec model, intrinsic extrinsic evaluations. cluster vectors relational similarity of words, word embeddings news article classifier extrinsic evaluation. skip-gram word2vec.
Fair Embedding Engine: Gender Bias Word Embeddings: Non-contextual word embedding, training corpora. syntactic semantic embeddings. Fair Embedding Engine (FEE) gender bias word embeddings. FEE gender bias word embeddings. FEE  embedding models.
All Word Embeddings from One Embedding: neural network-based models natural language processing (NLP), word embeddings. embedding matrix size vocabulary size. embeddings transforming shared embedding. ALONE allwordembeddingsfromone, constructed embedding feed-forward neural network. filter vectors conventional embedding matrix, vocabulary size.filter construction approach. ALONE can be used as word representation sufficiently through an experiment on the reconstruction of pre-trained word embeddings. In addition, we also conduct experiments on NLP application tasks: machine translation and summarization. We combined ALONE with the current state-of-the-art encoder-decoder model, the Transformer, and achieved comparable scores on WMT 2014 English-to-German translation and DUC 2004 very short summarization with less parameters.
PBoS: Probabilistic Bag-of-Subwords Word Embedding: word embeddings: word vectors finite vocabulary, embedding vectors vocabulary contextual information. subword segmentation subword-based word embedding. bag-of-subwords (PBoS) bag-of-subwords segmentations likelihood. PBoS subword segmentations subword rankings. Word similarity and POS tagging PBoS subword-level word embeddings languages.
Discrete Word Embedding Logical Natural Language Understanding: unsupervised neural model. binary embedding supports vector arithmetic operations continuous embeddings.
CogniFNN: Fuzzy Neural Network Framework Cognitive Word Embedding Evaluation: Word embeddings semantic representations, embedding qualities CogniFNN framework, fuzzy neural networks non-linear non-stationary characteristics EEG, fMRI, and eye-tracking, mean square error multiple hypotheses testing CogniFNN framework. CogniFNN context-independent (GloVe) context-sensitive (BERT) word embeddings.
Word embedding and neural network grammatical gender word embeddings grammatical gender Swedish. computational linguistics general linguistics. grammatical gender in language word embedding models artificial neural networks. previous linguistic hypotheses on assignment and usage of grammatical gender linguistic perspective.
Text Processing Sentiment Classification Word Embedding: text classification sentiment analysis. text domain (Twitter) and application (sentiment classification). Skip gram-based word embeddings Twitter colloquial words, emojis, and hashtag conventional literature corpora. sentiment classification.
Decomposing Word Embedding Capsule Network: Word sense disambiguation sense ambiguous word. pre-trained language multi-embeddings unsupervised word embedding. capsule network-based approach, image segmentation. Sense embedding, called CapsDecE2S. unsupervised ambiguous embedding capsule network multiple morpheme-like vectors, semantic language. attention operations, CapsDecE2S morpheme-like context-specific sense embedding. CapsDecE2S, sense matching. binary classification CapsDecE2S corpora Word-in-Context English all-words Word Sense Disambiguation, the CapsDecE2S word sense disambiguation tasks.

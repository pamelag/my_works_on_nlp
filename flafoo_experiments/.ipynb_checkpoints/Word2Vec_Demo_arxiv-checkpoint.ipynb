{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a word embedding from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def create_new_lines(dataset: list, item: str):\n",
    "    item = item.replace(\",\", \"\")\n",
    "    item = re.sub(r\"e.g.\", \"\", item)\n",
    "    item = re.sub(r\"i.e.\", \"\", item)\n",
    "    item = re.sub(r\"et al.\", \"\", item)\n",
    "    item = re.sub(r\"i.e.\", \"\", item)\n",
    "    item = re.sub(r\"i.e.,\", \"\", item)\n",
    "    item = re.sub(r\"vs.,\", \"\", item)\n",
    "    item = re.sub(r\"vs.\", \"\", item)\n",
    "    item = re.sub(r\"i. e.\", \"\", item)\n",
    "    lines = item.split(\". \")\n",
    "    dataset = [dataset.append(line) or line for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('data/arxiv.txt', 'r') \n",
    "fileContent = file1.readlines()\n",
    "# Use this dataset to add all the processed data\n",
    "dataset = []\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_processed_text = []\n",
    "\n",
    "print(len(fileContent))\n",
    "\n",
    "count = 0\n",
    "# Strips the newline character \n",
    "for line in fileContent: \n",
    "    count += 1\n",
    "    ## print(\"Line{}: {}\".format(count, line.strip()))\n",
    "    create_new_lines(dataset, line)\n",
    "    \n",
    "# to_lower(dataset)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(dataset:list):\n",
    "    lower_cased_lines = []\n",
    "    dataset = [dataset.append(line.lower()) or line for line in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    }
   ],
   "source": [
    "file1 = open('data/arxiv.txt', 'r') \n",
    "fileContent = file1.readlines()\n",
    "# Use this dataset to add all the processed data\n",
    "dataset = []\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_processed_text = []\n",
    "\n",
    "print(len(fileContent))\n",
    "\n",
    "count = 0\n",
    "# Strips the newline character \n",
    "for line in fileContent: \n",
    "    count += 1\n",
    "    ## print(\"Line{}: {}\".format(count, line.strip()))\n",
    "    create_new_lines(dataset, line)\n",
    "    \n",
    "# to_lower(dataset)\n",
    "# print(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "# df.head()\n",
    "\n",
    "df.to_csv(\"arxiv.csv\", header='Text')\n",
    "\n",
    "# with open(\"arxiv.csv\", 'w', newline='') as myfile:\n",
    "#         wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#         for line in dataset: \n",
    "#             wr.writerow(line)\n",
    "## Tomorrow I am going to remove all the commas and will create a new line for all the periods encountered in a sentence.\n",
    "## This is going to be an experiment and I am okay with it, need to see what would be the result.\n",
    "## print(dataset)\n",
    "# print(all_processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(dataset):\n",
    "    for sentence in dataset:\n",
    "        # Cleaning the text\n",
    "        text = text_preprocessing(sentence)\n",
    "        all_processed_text.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing needs to be rewritten using functional python - I am keen to use pipeline\n",
    "def text_preprocessing(\n",
    "    text:list,\n",
    "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
    "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
    "    )->list:\n",
    "    \"\"\"\n",
    "    A method to preproces text\n",
    "    \"\"\"\n",
    "    for x in text.lower(): \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Removing digits\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Converting all our text to a list \n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Droping empty strings\n",
    "    text = [x for x in text if x!='']\n",
    "\n",
    "    # Droping stop words\n",
    "    text = [x for x in text if x not in stop_words]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def create_unique_word_dict(text:list) -> dict:\n",
    "    \"\"\"\n",
    "    A method that creates a dictionary where the keys are unique words\n",
    "    and key values are indices\n",
    "    \"\"\"\n",
    "    # Getting all the unique words from our text and sorting them alphabetically\n",
    "    words = list(set(text))\n",
    "    words.sort()\n",
    "\n",
    "    # Creating the dictionary for the unique words\n",
    "    unique_word_dict = {}\n",
    "    for i, word in enumerate(words):\n",
    "        unique_word_dict.update({\n",
    "            word: i\n",
    "        })\n",
    "\n",
    "    return unique_word_dict    \n",
    "\n",
    "def text_preprocessing(\n",
    "    text:list,\n",
    "    punctuations = r'''!()-[]{};:'\"\\,<>./?@#$%^&*_“~''',\n",
    "    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n",
    "    )->list:\n",
    "    \"\"\"\n",
    "    A method to preproces text\n",
    "    \"\"\"\n",
    "    for x in text.lower(): \n",
    "        if x in punctuations: \n",
    "            text = text.replace(x, \"\")\n",
    "\n",
    "    # Removing words that have numbers in them\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Removing digits\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Setting every word to lower\n",
    "    text = text.lower()\n",
    "\n",
    "    # Converting all our text to a list \n",
    "    text = text.split(' ')\n",
    "\n",
    "    # Droping empty strings\n",
    "    text = [x for x in text if x!='']\n",
    "\n",
    "    # Droping stop words\n",
    "    text = [x for x in text if x not in stop_words]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Functions to find the most similar word \n",
    "def euclidean(vec1:np.array, vec2:np.array) -> float:\n",
    "    \"\"\"\n",
    "    A function to calculate the euclidean distance between two vectors\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum((vec1 - vec2)**2))\n",
    "\n",
    "def find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n",
    "    \"\"\"\n",
    "    A method to find the most similar word based on the learnt embeddings\n",
    "    \"\"\"\n",
    "    dist_dict = {}\n",
    "    word_vector = embedding_dict.get(word, [])\n",
    "    if len(word_vector) > 0:\n",
    "        for key, value in embedding_dict.items():\n",
    "            if key!=word:\n",
    "                dist = euclidean(word_vector, value)\n",
    "                dist_dict.update({\n",
    "                    key: dist\n",
    "                })\n",
    "\n",
    "        return sorted(dist_dict.items(), key=lambda x: x[1])[0:top_n]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Drawing the embeddings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Deep learning: \n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the text from the input folder\n",
    "texts = pd.read_csv('data/arxiv.csv')\n",
    "texts = [x for x in texts['text']]\n",
    "\n",
    "# Defining the window for context\n",
    "window = 2\n",
    "\n",
    "# Creating a placeholder for the scanning of the word list\n",
    "word_lists = []\n",
    "all_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "\n",
    "    # Cleaning the text\n",
    "    text = text_preprocessing(text)\n",
    "\n",
    "    # Appending to the all text list\n",
    "    all_text += text \n",
    "\n",
    "    # Creating a context dictionary\n",
    "    for i, word in enumerate(text):\n",
    "        for w in range(window):\n",
    "            # Getting the context that is ahead by *window* words\n",
    "            if i + 1 + w < len(text): \n",
    "                word_lists.append([word] + [text[(i + 1 + w)]])\n",
    "            # Getting the context that is behind by *window* words    \n",
    "            if i - w - 1 >= 0:\n",
    "                word_lists.append([word] + [text[(i - w - 1)]])\n",
    "\n",
    "unique_word_dict = create_unique_word_dict(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of features (unique words)\n",
    "n_words = len(unique_word_dict)\n",
    "\n",
    "# Getting all the unique words \n",
    "words = list(unique_word_dict.keys())\n",
    "\n",
    "# Creating the X and Y matrices using one hot encoding\n",
    "X = []\n",
    "Y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word_list in tqdm(enumerate(word_lists)):\n",
    "    # Getting the indices\n",
    "    main_word_index = unique_word_dict.get(word_list[0])\n",
    "    context_word_index = unique_word_dict.get(word_list[1])\n",
    "\n",
    "    # Creating the placeholders   \n",
    "    X_row = np.zeros(n_words)\n",
    "    Y_row = np.zeros(n_words)\n",
    "\n",
    "    # One hot encoding the main word\n",
    "    X_row[main_word_index] = 1\n",
    "\n",
    "    # One hot encoding the Y matrix words \n",
    "    Y_row[context_word_index] = 1\n",
    "\n",
    "    # Appending to the main matrices\n",
    "    X.append(X_row)\n",
    "    Y.append(Y_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the matrices into a sparse format because the vast majority of the data are 0s\n",
    "X = sparse.csr_matrix(X)\n",
    "Y = sparse.csr_matrix(Y)\n",
    "\n",
    "# Defining the size of the embedding\n",
    "embed_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the neural network\n",
    "inp = Input(shape=(X.shape[1],))\n",
    "x = Dense(units=embed_size, activation='linear')(inp)\n",
    "x = Dense(units=Y.shape[1], activation='softmax')(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "# Optimizing the network weights\n",
    "model.fit(\n",
    "    x=X, \n",
    "    y=Y, \n",
    "    batch_size=256,\n",
    "    epochs=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the weights from the neural network. \n",
    "# These are the so called word embeddings\n",
    "\n",
    "# The input layer \n",
    "weights = model.get_weights()[0]\n",
    "\n",
    "# Creating a dictionary to store the embeddings in. The key is a unique word and \n",
    "# the value is the numeric vector\n",
    "embedding_dict = {}\n",
    "for word in words: \n",
    "    embedding_dict.update({\n",
    "        word: weights[unique_word_dict.get(word)]\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting the embeddings\n",
    "plt.figure(figsize=(10, 10))\n",
    "for word in list(unique_word_dict.keys()):\n",
    "    coord = embedding_dict.get(word)\n",
    "    plt.scatter(coord[0], coord[1])\n",
    "    plt.annotate(word, (coord[0], coord[1]))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the embedding vector to a txt file\n",
    "try:\n",
    "    os.mkdir(f'{os.getcwd()}\\\\output')        \n",
    "except Exception as e:\n",
    "    print(f'Cannot create output folder: {e}')\n",
    "\n",
    "with open(f'{os.getcwd()}\\\\output\\\\embedding.txt', 'w') as f:\n",
    "    for key, value in embedding_dict.items():\n",
    "        try:\n",
    "            f.write(f'{key}: {value}\\n')   \n",
    "        except Exception as e:\n",
    "            print(f'Cannot write word {key} to dict: {e}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\"The dominant sequence transduction models are based on complex recurrent or \" +\n",
    "\"convolutional neural networks that include an encoder and a decoder. The best \" +\n",
    "\"performing models also connect the encoder and decoder through an attention \" +\n",
    "\"mechanism. We propose a new simple network architecture, the Transformer, \" +\n",
    "\"based solely on attention mechanisms, dispensing with recurrence and convolutions \" +\n",
    "\"entirely. Experiments on two machine translation tasks show these models to \" +\n",
    "\"be superior in quality while being more parallelizable and requiring significantly \" +\n",
    "\"less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German \" +\n",
    "\"translation task, improving over the existing best results, including \" +\n",
    "\"ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, \" +\n",
    "\"our model establishes a new single-model state-of-the-art BLEU score of 41.8 after \" +\n",
    "\"training for 3.5 days on eight GPUs, a small fraction of the training costs of the \" +\n",
    "\"best models from the literature. We show that the Transformer generalizes well to \" +\n",
    "\"other tasks by applying it successfully to English constituency parsing both with \" +\n",
    "\"large and limited training data. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [[nltk.word_tokenize(w), ' '] for w in text.split()]\n",
    "ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = nltk.pos_tag(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The dominant sequence transduction models are based on complex recurrent or \" +\n",
    "\"convolutional neural networks that include an encoder and a decoder. The best \" +\n",
    "\"performing models also connect the encoder and decoder through an attention \" +\n",
    "\"mechanism. We propose a new simple network architecture, the Transformer, \" +\n",
    "\"based solely on attention mechanisms, dispensing with recurrence and convolutions \" +\n",
    "\"entirely. Experiments on two machine translation tasks show these models to \" +\n",
    "\"be superior in quality while being more parallelizable and requiring significantly \" +\n",
    "\"less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German \" +\n",
    "\"translation task, improving over the existing best results, including \" +\n",
    "\"ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, \" +\n",
    "\"our model establishes a new single-model state-of-the-art BLEU score of 41.8 after \" +\n",
    "\"training for 3.5 days on eight GPUs, a small fraction of the training costs of the \" +\n",
    "\"best models from the literature. We show that the Transformer generalizes well to \" +\n",
    "\"other tasks by applying it successfully to English constituency parsing both with \" +\n",
    "\"large and limited training data. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"The dominant sequence transduction models are based on complex recurrent or \" +\n",
    "\"convolutional neural networks that include an encoder and a decoder. The best \" +\n",
    "\"performing models also connect the encoder and decoder through an attention \" +\n",
    "\"mechanism. We propose a new simple network architecture, the Transformer, \" +\n",
    "\"based solely on attention mechanisms, dispensing with recurrence and convolutions \" +\n",
    "\"entirely. Experiments on two machine translation tasks show these models to \" +\n",
    "\"be superior in quality while being more parallelizable and requiring significantly \" +\n",
    "\"less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German \" +\n",
    "\"translation task, improving over the existing best results, including \" +\n",
    "\"ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, \" +\n",
    "\"our model establishes a new single-model state-of-the-art BLEU score of 41.8 after \" +\n",
    "\"training for 3.5 days on eight GPUs, a small fraction of the training costs of the \" +\n",
    "\"best models from the literature. We show that the Transformer generalizes well to \" +\n",
    "\"other tasks by applying it successfully to English constituency parsing both with \" +\n",
    "\"large and limited training data. \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer 291 302 ORG\n",
      "two 411 414 CARDINAL\n",
      "28.4 584 588 CARDINAL\n",
      "2014 Englishto-German 605 626 PERSON\n",
      "English 740 747 LANGUAGE\n",
      "French 751 757 NORP\n",
      "BLEU 834 838 ORG\n",
      "41.8 848 852 CARDINAL\n",
      "3.5 days 872 880 DATE\n",
      "eight 884 889 CARDINAL\n",
      "Transformer 992 1003 PERSON\n",
      "English 1067 1074 NORP\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT det Xxx True True\n",
      "dominant dominant ADJ JJ amod xxxx True False\n",
      "sequence sequence NOUN NN compound xxxx True False\n",
      "transduction transduction NOUN NN compound xxxx True False\n",
      "models model NOUN NNS nsubjpass xxxx True False\n",
      "are be VERB VBP auxpass xxx True True\n",
      "based base VERB VBN ROOT xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "complex complex ADJ JJ amod xxxx True False\n",
      "recurrent recurrent NOUN NN amod xxxx True False\n",
      "or or CCONJ CC cc xx True True\n",
      "convolutional convolutional ADJ JJ conj xxxx True False\n",
      "neural neural ADJ JJ amod xxxx True False\n",
      "networks network NOUN NNS pobj xxxx True False\n",
      "that that DET WDT nsubj xxxx True True\n",
      "include include VERB VBP relcl xxxx True False\n",
      "an an DET DT det xx True True\n",
      "encoder encoder NOUN NN dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "a a DET DT det x True True\n",
      "decoder decoder NOUN NN conj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True True\n",
      "best best ADV RBS amod xxxx True False\n",
      "performing perform VERB VBG amod xxxx True False\n",
      "models model NOUN NNS nsubj xxxx True False\n",
      "also also ADV RB advmod xxxx True True\n",
      "connect connect VERB VBP ROOT xxxx True False\n",
      "the the DET DT det xxx True True\n",
      "encoder encoder NOUN NN dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "decoder decoder VERB VB conj xxxx True False\n",
      "through through ADP IN prep xxxx True True\n",
      "an an DET DT det xx True True\n",
      "attention attention NOUN NN compound xxxx True False\n",
      "mechanism mechanism NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "We -PRON- PRON PRP nsubj Xx True True\n",
      "propose propose VERB VBP ROOT xxxx True False\n",
      "a a DET DT det x True True\n",
      "new new ADJ JJ amod xxx True False\n",
      "simple simple ADJ JJ amod xxxx True False\n",
      "network network NOUN NN compound xxxx True False\n",
      "architecture architecture NOUN NN dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "Transformer Transformer PROPN NNP appos Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "based base VERB VBN acl xxxx True False\n",
      "solely solely ADV RB advmod xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "attention attention NOUN NN compound xxxx True False\n",
      "mechanisms mechanism NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "dispensing dispense VERB VBG advcl xxxx True False\n",
      "with with ADP IN prep xxxx True True\n",
      "recurrence recurrence NOUN NN pobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "convolutions convolution NOUN NNS conj xxxx True False\n",
      "entirely entirely ADV RB advmod xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Experiments experiment NOUN NNS nsubj Xxxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "two two NUM CD nummod xxx True True\n",
      "machine machine NOUN NN compound xxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "tasks task NOUN NNS pobj xxxx True False\n",
      "show show VERB VBP ROOT xxxx True True\n",
      "these these DET DT det xxxx True True\n",
      "models model NOUN NNS nsubj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "be be VERB VB ccomp xx True True\n",
      "superior superior ADJ JJ acomp xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "quality quality NOUN NN pobj xxxx True False\n",
      "while while ADP IN mark xxxx True True\n",
      "being be VERB VBG advcl xxxx True True\n",
      "more more ADV RBR advmod xxxx True True\n",
      "parallelizable parallelizable ADJ JJ acomp xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "requiring require VERB VBG conj xxxx True False\n",
      "significantly significantly ADV RB advmod xxxx True False\n",
      "less less ADJ JJR amod xxxx True True\n",
      "time time NOUN NN dobj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "train train VERB VB relcl xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Our -PRON- DET PRP$ poss Xxx True True\n",
      "model model NOUN NN nsubj xxxx True False\n",
      "achieves achieve VERB VBZ ROOT xxxx True False\n",
      "28.4 28.4 NUM CD nummod dd.d False False\n",
      "BLEU BLEU PROPN NNP dobj XXXX True False\n",
      "on on ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "WMT WMT PROPN NNP nmod XXX True False\n",
      "2014 2014 NUM CD nummod dddd False False\n",
      "Englishto englishto ADJ JJ compound Xxxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "German german ADJ JJ amod Xxxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "task task NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "improving improve VERB VBG advcl xxxx True False\n",
      "over over ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "existing exist VERB VBG amod xxxx True False\n",
      "best good ADJ JJS amod xxxx True False\n",
      "results result NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "including include VERB VBG prep xxxx True False\n",
      "ensembles ensemble NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "by by ADP IN prep xx True True\n",
      "over over ADP IN quantmod xxxx True True\n",
      "2 2 NUM CD nummod d False False\n",
      "BLEU BLEU PROPN NNP pobj XXXX True False\n",
      ". . PUNCT . punct . False False\n",
      "On on ADP IN prep Xx True True\n",
      "the the DET DT det xxx True True\n",
      "WMT WMT PROPN NNP nmod XXX True False\n",
      "2014 2014 NUM CD nummod dddd False False\n",
      "English English PROPN NNP nmod Xxxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "to to ADP IN prep xx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "French french ADJ JJ pobj Xxxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "task task NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "our -PRON- DET PRP$ poss xxx True True\n",
      "model model NOUN NN nsubj xxxx True False\n",
      "establishes establish VERB VBZ ROOT xxxx True False\n",
      "a a DET DT det x True True\n",
      "new new ADJ JJ amod xxx True False\n",
      "single single ADJ JJ amod xxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "model model NOUN NN compound xxxx True False\n",
      "state state NOUN NN nmod xxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "of of ADP IN prep xx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "the the DET DT det xxx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "art art NOUN NN pobj xxx True False\n",
      "BLEU BLEU PROPN NNP compound XXXX True False\n",
      "score score NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "41.8 41.8 NUM CD pobj dd.d False False\n",
      "after after ADP IN prep xxxx True True\n",
      "training train VERB VBG pcomp xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "3.5 3.5 NUM CD nummod d.d False False\n",
      "days day NOUN NNS pobj xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "eight eight NUM CD nummod xxxx True True\n",
      "GPUs gpu NOUN NNS pobj XXXx True False\n",
      ", , PUNCT , punct , False False\n",
      "a a DET DT det x True True\n",
      "small small ADJ JJ amod xxxx True False\n",
      "fraction fraction NOUN NN appos xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "training training NOUN NN compound xxxx True False\n",
      "costs cost NOUN NNS pobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "best good ADJ JJS amod xxxx True False\n",
      "models model NOUN NNS pobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "literature literature NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "We -PRON- PRON PRP nsubj Xx True True\n",
      "show show VERB VBP ROOT xxxx True True\n",
      "that that ADP IN mark xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "Transformer Transformer PROPN NNP nsubj Xxxxx True False\n",
      "generalizes generalize VERB VBZ ccomp xxxx True False\n",
      "well well ADV RB advmod xxxx True True\n",
      "to to ADP IN prep xx True True\n",
      "other other ADJ JJ amod xxxx True True\n",
      "tasks task NOUN NNS pobj xxxx True False\n",
      "by by ADP IN prep xx True True\n",
      "applying apply VERB VBG pcomp xxxx True False\n",
      "it -PRON- PRON PRP dobj xx True True\n",
      "successfully successfully ADV RB advmod xxxx True False\n",
      "to to ADP IN prep xx True True\n",
      "English english ADJ JJ amod Xxxxx True False\n",
      "constituency constituency NOUN NN pobj xxxx True False\n",
      "parsing parse VERB VBG advcl xxxx True False\n",
      "both both DET DT dobj xxxx True True\n",
      "with with ADP IN prep xxxx True True\n",
      "large large ADJ JJ amod xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "limited limited ADJ JJ conj xxxx True False\n",
      "training training NOUN NN compound xxxx True False\n",
      "data datum NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The the DET DT det Xxx True True\n",
      "dominant dominant ADJ JJ amod xxxx True False\n",
      "sequence sequence NOUN NN compound xxxx True False\n",
      "transduction transduction NOUN NN compound xxxx True False\n",
      "models model NOUN NNS nsubjpass xxxx True False\n",
      "are be VERB VBP auxpass xxx True True\n",
      "based base VERB VBN ROOT xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "complex complex ADJ JJ amod xxxx True False\n",
      "recurrent recurrent NOUN NN amod xxxx True False\n",
      "or or CCONJ CC cc xx True True\n",
      "convolutional convolutional ADJ JJ conj xxxx True False\n",
      "neural neural ADJ JJ amod xxxx True False\n",
      "networks network NOUN NNS pobj xxxx True False\n",
      "that that DET WDT nsubj xxxx True True\n",
      "include include VERB VBP relcl xxxx True False\n",
      "an an DET DT det xx True True\n",
      "encoder encoder NOUN NN dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "a a DET DT det x True True\n",
      "decoder decoder NOUN NN conj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "The the DET DT det Xxx True True\n",
      "best best ADV RBS amod xxxx True False\n",
      "performing perform VERB VBG amod xxxx True False\n",
      "models model NOUN NNS nsubj xxxx True False\n",
      "also also ADV RB advmod xxxx True True\n",
      "connect connect VERB VBP ROOT xxxx True False\n",
      "the the DET DT det xxx True True\n",
      "encoder encoder NOUN NN dobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "decoder decoder VERB VB conj xxxx True False\n",
      "through through ADP IN prep xxxx True True\n",
      "an an DET DT det xx True True\n",
      "attention attention NOUN NN compound xxxx True False\n",
      "mechanism mechanism NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "We -PRON- PRON PRP nsubj Xx True True\n",
      "propose propose VERB VBP ROOT xxxx True False\n",
      "a a DET DT det x True True\n",
      "new new ADJ JJ amod xxx True False\n",
      "simple simple ADJ JJ amod xxxx True False\n",
      "network network NOUN NN compound xxxx True False\n",
      "architecture architecture NOUN NN dobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "the the DET DT det xxx True True\n",
      "Transformer Transformer PROPN NNP appos Xxxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "based base VERB VBN acl xxxx True False\n",
      "solely solely ADV RB advmod xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "attention attention NOUN NN compound xxxx True False\n",
      "mechanisms mechanism NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "dispensing dispense VERB VBG advcl xxxx True False\n",
      "with with ADP IN prep xxxx True True\n",
      "recurrence recurrence NOUN NN pobj xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "convolutions convolution NOUN NNS conj xxxx True False\n",
      "entirely entirely ADV RB advmod xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Experiments experiment NOUN NNS nsubj Xxxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "two two NUM CD nummod xxx True True\n",
      "machine machine NOUN NN compound xxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "tasks task NOUN NNS pobj xxxx True False\n",
      "show show VERB VBP ROOT xxxx True True\n",
      "these these DET DT det xxxx True True\n",
      "models model NOUN NNS nsubj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "be be VERB VB ccomp xx True True\n",
      "superior superior ADJ JJ acomp xxxx True False\n",
      "in in ADP IN prep xx True True\n",
      "quality quality NOUN NN pobj xxxx True False\n",
      "while while ADP IN mark xxxx True True\n",
      "being be VERB VBG advcl xxxx True True\n",
      "more more ADV RBR advmod xxxx True True\n",
      "parallelizable parallelizable ADJ JJ acomp xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "requiring require VERB VBG conj xxxx True False\n",
      "significantly significantly ADV RB advmod xxxx True False\n",
      "less less ADJ JJR amod xxxx True True\n",
      "time time NOUN NN dobj xxxx True False\n",
      "to to PART TO aux xx True True\n",
      "train train VERB VB relcl xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "Our -PRON- DET PRP$ poss Xxx True True\n",
      "model model NOUN NN nsubj xxxx True False\n",
      "achieves achieve VERB VBZ ROOT xxxx True False\n",
      "28.4 28.4 NUM CD nummod dd.d False False\n",
      "BLEU BLEU PROPN NNP dobj XXXX True False\n",
      "on on ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "WMT WMT PROPN NNP nmod XXX True False\n",
      "2014 2014 NUM CD nummod dddd False False\n",
      "Englishto englishto ADJ JJ compound Xxxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "German german ADJ JJ amod Xxxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "task task NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "improving improve VERB VBG advcl xxxx True False\n",
      "over over ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "existing exist VERB VBG amod xxxx True False\n",
      "best good ADJ JJS amod xxxx True False\n",
      "results result NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "including include VERB VBG prep xxxx True False\n",
      "ensembles ensemble NOUN NNS pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "by by ADP IN prep xx True True\n",
      "over over ADP IN quantmod xxxx True True\n",
      "2 2 NUM CD nummod d False False\n",
      "BLEU BLEU PROPN NNP pobj XXXX True False\n",
      ". . PUNCT . punct . False False\n",
      "On on ADP IN prep Xx True True\n",
      "the the DET DT det xxx True True\n",
      "WMT WMT PROPN NNP nmod XXX True False\n",
      "2014 2014 NUM CD nummod dddd False False\n",
      "English English PROPN NNP nmod Xxxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "to to ADP IN prep xx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "French french ADJ JJ pobj Xxxxx True False\n",
      "translation translation NOUN NN compound xxxx True False\n",
      "task task NOUN NN pobj xxxx True False\n",
      ", , PUNCT , punct , False False\n",
      "our -PRON- DET PRP$ poss xxx True True\n",
      "model model NOUN NN nsubj xxxx True False\n",
      "establishes establish VERB VBZ ROOT xxxx True False\n",
      "a a DET DT det x True True\n",
      "new new ADJ JJ amod xxx True False\n",
      "single single ADJ JJ amod xxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "model model NOUN NN compound xxxx True False\n",
      "state state NOUN NN nmod xxxx True False\n",
      "- - PUNCT HYPH punct - False False\n",
      "of of ADP IN prep xx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "the the DET DT det xxx True True\n",
      "- - PUNCT HYPH punct - False False\n",
      "art art NOUN NN pobj xxx True False\n",
      "BLEU BLEU PROPN NNP compound XXXX True False\n",
      "score score NOUN NN dobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "41.8 41.8 NUM CD pobj dd.d False False\n",
      "after after ADP IN prep xxxx True True\n",
      "training train VERB VBG pcomp xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "3.5 3.5 NUM CD nummod d.d False False\n",
      "days day NOUN NNS pobj xxxx True False\n",
      "on on ADP IN prep xx True True\n",
      "eight eight NUM CD nummod xxxx True True\n",
      "GPUs gpu NOUN NNS pobj XXXx True False\n",
      ", , PUNCT , punct , False False\n",
      "a a DET DT det x True True\n",
      "small small ADJ JJ amod xxxx True False\n",
      "fraction fraction NOUN NN appos xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "training training NOUN NN compound xxxx True False\n",
      "costs cost NOUN NNS pobj xxxx True False\n",
      "of of ADP IN prep xx True True\n",
      "the the DET DT det xxx True True\n",
      "best good ADJ JJS amod xxxx True False\n",
      "models model NOUN NNS pobj xxxx True False\n",
      "from from ADP IN prep xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "literature literature NOUN NN pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n",
      "We -PRON- PRON PRP nsubj Xx True True\n",
      "show show VERB VBP ROOT xxxx True True\n",
      "that that ADP IN mark xxxx True True\n",
      "the the DET DT det xxx True True\n",
      "Transformer Transformer PROPN NNP nsubj Xxxxx True False\n",
      "generalizes generalize VERB VBZ ccomp xxxx True False\n",
      "well well ADV RB advmod xxxx True True\n",
      "to to ADP IN prep xx True True\n",
      "other other ADJ JJ amod xxxx True True\n",
      "tasks task NOUN NNS pobj xxxx True False\n",
      "by by ADP IN prep xx True True\n",
      "applying apply VERB VBG pcomp xxxx True False\n",
      "it -PRON- PRON PRP dobj xx True True\n",
      "successfully successfully ADV RB advmod xxxx True False\n",
      "to to ADP IN prep xx True True\n",
      "English english ADJ JJ amod Xxxxx True False\n",
      "constituency constituency NOUN NN pobj xxxx True False\n",
      "parsing parse VERB VBG advcl xxxx True False\n",
      "both both DET DT dobj xxxx True True\n",
      "with with ADP IN prep xxxx True True\n",
      "large large ADJ JJ amod xxxx True False\n",
      "and and CCONJ CC cc xxx True True\n",
      "limited limited ADJ JJ conj xxxx True False\n",
      "training training NOUN NN compound xxxx True False\n",
      "data datum NOUN NNS pobj xxxx True False\n",
      ". . PUNCT . punct . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
